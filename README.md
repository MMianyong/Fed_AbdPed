# Fed_AbdPed
The final FL model, nnU-Net plans, and the implementation code for nnU-Net with NVFlare using SURFdrive as the intermediary  will be shared latter.


NVFLARE setup and application structure
To use NVFLARE, two main components are required: the secure startup packages and the job definitions.
First, startup packages were generated to enable communication between the server and clients. These packages define the federation topology, including the number of clients, the server, the admin user, and the server IP address. The configuration was specified in a project.yaml file, adapted from the official NVFLARE example  , and modified to include one server, two clients, and one admin. The startup packages were generated using the nvflare provision command. After provisioning, separate folders for the server and each client were created, and each role was launched in a separate terminal to establish a secure federated learning session.
Second, the jobs folder was prepared to define the federated training configuration for both the server and the clients. Each job is stored in a separate subfolder and contains the required scripts and configuration files. In this study, we used the NVFLARE client API, and the main federated learning logic and settings were defined within the job folder. Most settings followed the default NVFLARE client API configuration .
Once the session was established, the admin used the FLARE API to connect to the federation and submit training jobs. , thereby initiating federated model training.
Training configuration and learning rate strategy
A polynomial learning rate schedule was used, with an initial learning rate of 0.01 that decayed over epochs. To remain consistent with nnU-Net’s default training strategy, the initial learning rate was fixed at 0.01 for federated learning. For each federated round, the effective epoch number was accumulated across previous rounds, allowing the learning rate to be adjusted in a manner equivalent to centralized training.
Before training started, a randomly initialized model was distributed to both centers. This same initialization was used for both federated and non-federated training to ensure a fair comparison. In NVFLARE, this was achieved by configuring the YAML files to load the initial model from a local directory.
NVFLARE client workflow and aggregation
In the standard NVFLARE client API workflow, each client receives the global model from the server along with configuration information for the current round, including the total number of epochs per round and the current round index. The client loads the received model parameters, adjusts the learning rate according to the accumulated epoch count, and performs local training for a fixed number of epochs. The updated model is then sent back to the server together with metadata used for aggregation.
The default NVFLARE controller (nvflare.app_common.workflows.fedavg.FedAvg) was used. Aggregation was performed using FedAvg, where the aggregation weights were determined by the number of training samples reported by each client (by transferring the MetaKey below to the local learner ("MetaKey.NUM_STEPS_CURRENT_ROUND")). No model selection was applied; instead, the final model from the last round and epoch was used for evaluation, consistent with nnU-Net’s recommended practice for local training.
Integration with SURFdrive for cross-network training
Due to network constraints, the server and two logical clients were deployed on the same machine within the UMC UTR (UMCU) network: a UTR client, a pseudo-HEI client, and the server. The UTR client followed the standard NVFLARE workflow. The pseudo-HEI client acted as an intermediary to enable communication with the real HEI client located within the DKFZ network.
For each federated round, the pseudo-HEI client received the global model weights from the server and uploaded them to SURFdrive. The real HEI client, operating outside the NVFLARE framework but following the same client API logic, downloaded the model weights from SURFdrive, performed local training using its own dataset, and uploaded the updated weights back to SURFdrive. The pseudo-HEI client then downloaded the updated model from SURFdrive and forwarded it to the server.
Model transfer via SURFdrive was automated using the WebDAV protocol, with upload and download operations implemented through curl commands. Once the server received model updates from both the UTR client and the pseudo-HEI client, it performed FedAvg aggregation based on the reported sample counts and broadcast the updated global model to the clients to start the next round. The real HEI client then waited for the next global model to be available on SURFdrive before continuing training.
This strategy avoids the need to involve the IT departments at both centers to open network ports for direct communication. This process can be time-consuming and may raise security concerns. Because the real HEI client does not rely on the NVFLARE framework and NVFLARE is deployed only on the UTR server, the overall system dependency is reduced. This design also allows the framework to scale easily to additional centers, as new clients can be integrated with minimal setup.
The main trade-off of this approach is a potential increase in upload and download time. However, in practice, the model upload and download times via SURFdrive were within 5 seconds. In comparison, model training at the HEI center required approximately 3 minutes per epoch. Given the training time, the additional communication overhead was negligible.
<img width="471" height="648" alt="image" src="https://github.com/user-attachments/assets/6aac734d-3146-4012-bb3e-65faab8b68f6" />
